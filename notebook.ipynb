{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JXya_77BxPb"
   },
   "source": [
    "# Optimisation des prix avec l'apprentissage par renforcement (DQN)\n",
    "\n",
    "## Contexte du projet\n",
    "Nous voulons trouver la meilleure stratégie de prix pour maximiser les profits d'un produit.  \n",
    "Le marché est dynamique : les clients réagissent aux prix actuels **et** aux prix passés.  \n",
    "Les méthodes classiques (prix fixe ou ajustement simple) ne suffisent pas.  \n",
    "Nous allons utiliser un algorithme de renforcement profond (DQN) pour s’adapter à cet environnement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxD35ROgAQ_7"
   },
   "outputs": [],
   "source": [
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib import rc, animation\n",
    "from qbstyles import mpl_style\n",
    "from bokeh.io import output_notebook\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "plt.rcParams.update({'pdf.fonttype': 'truetype'})\n",
    "\n",
    "mpl_style(dark=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USS7CxoaLdit"
   },
   "source": [
    "\n",
    "## Modélisation du marché : environnement simulé\n",
    "Nous créons un environnement qui simule la réaction des clients.  \n",
    "Cet environnement prend en compte :  \n",
    "- L’effet de mémoire : les prix passés influencent les ventes futures.  \n",
    "- La réponse asymétrique : baisse et hausse de prix n’ont pas le même effet.  \n",
    "\n",
    "Cela nous permettra de tester différentes stratégies de prix en toute sécurité.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "oSJp6Bff6f92",
    "outputId": "ae54e092-24c2-4230-bd24-51a46568a573"
   },
   "outputs": [],
   "source": [
    "## Environment simulator\n",
    "def plus(x):\n",
    "    return 0 if x < 0 else x\n",
    "\n",
    "def minus(x):\n",
    "    return 0 if x > 0 else -x\n",
    "\n",
    "def shock(x):\n",
    "    return np.sqrt(x)\n",
    "\n",
    "# Demand at time step t for current price p_t and previous price p_t_1\n",
    "def q_t(p_t, p_t_1, q_0, k, a, b):\n",
    "    return plus(q_0 - k*p_t - a*shock(plus(p_t - p_t_1)) + b*shock(minus(p_t - p_t_1)))\n",
    "\n",
    "# Profit at time step t\n",
    "def profit_t(p_t, p_t_1, q_0, k, a, b, unit_cost):\n",
    "    return q_t(p_t, p_t_1, q_0, k, a, b)*(p_t - unit_cost) \n",
    "\n",
    "# Total profit for price vector p over len(p) time steps\n",
    "def profit_total(p, unit_cost, q_0, k, a, b):\n",
    "    return profit_t(p[0], p[0], q_0, k, 0, 0, unit_cost) + sum(map(lambda t: profit_t(p[t], p[t-1], q_0, k, a, b, unit_cost), range(len(p))))\n",
    "\n",
    "## Environment parameters\n",
    "T = 20\n",
    "price_max = 500\n",
    "price_step = 10\n",
    "q_0 = 5000\n",
    "k = 20\n",
    "unit_cost = 100\n",
    "a_q = 300\n",
    "b_q = 100\n",
    "\n",
    "## Partial bindings for readability\n",
    "def profit_t_response(p_t, p_t_1):\n",
    "    return profit_t(p_t, p_t_1, q_0, k, a_q, b_q, unit_cost)\n",
    "\n",
    "def profit_response(p):\n",
    "    return profit_total(p, unit_cost, q_0, k, a_q, b_q)\n",
    "\n",
    "## Visualize price-demand functions\n",
    "price_grid = np.arange(price_step, price_max, price_step)\n",
    "price_change_grid = np.arange(0.5, 2.0, 0.1)\n",
    "profit_map = np.zeros( (len(price_grid), len(price_change_grid)) )\n",
    "for i in range(len(price_grid)):\n",
    "    for j in range(len(price_change_grid)):\n",
    "        profit_map[i,j] = profit_t_response(price_grid[i], price_grid[i]*price_change_grid[j])\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "for i in range(len(price_change_grid)):\n",
    "    if math.isclose(price_change_grid[i], 1.0):\n",
    "        color = 'black'\n",
    "    else:\n",
    "        p_norm = (price_change_grid[i]-0.5)/1.5\n",
    "        color = (p_norm, 0.4, 1 - p_norm)\n",
    "    plt.plot(price_grid, profit_map[:, i], c=color)\n",
    "plt.xlabel(\"Price ($)\")\n",
    "plt.ylabel(\"Profit\")\n",
    "plt.legend(np.int_(np.round((1-price_change_grid)*100)), loc='lower right', title=\"Price change (%)\", fancybox=False, framealpha=0.6)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pKl6HK06SBvP"
   },
   "source": [
    "## Stratégies de base (baselines)\n",
    "Avant d’utiliser l’algorithme de renforcement, on teste deux stratégies simples :  \n",
    "1️⃣ **Prix constant optimal** : trouver un prix unique qui maximise le profit moyen.  \n",
    "2️⃣ **Recherche séquentielle (greedy)** : ajuster le prix à chaque période pour améliorer les profits.\n",
    "\n",
    "Ces stratégies simples serviront de référence pour comparer l’efficacité de notre DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jUcq7PJJEj_n",
    "outputId": "b1b0fe99-4354-4430-e2a5-06b2daee91b7"
   },
   "outputs": [],
   "source": [
    "# Find optimal constant price\n",
    "\n",
    "profits = np.array([ profit_response(np.repeat(p, T)) for p in price_grid ])\n",
    "p_idx = np.argmax(profits)\n",
    "price_opt_const = price_grid[p_idx]\n",
    "\n",
    "print(f'Optimal price is {price_opt_const}, achieved profit is {profits[p_idx]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "STcRL-pNcsWP",
    "outputId": "a9ab6890-7f8d-4b96-8578-270bc8f2433d"
   },
   "outputs": [],
   "source": [
    "# Find optimal sequence of prices using greedy search\n",
    "\n",
    "def find_optimal_price_t(p_baseline, price_grid, t):\n",
    "    p_grid = np.tile(p_baseline, (len(price_grid), 1))\n",
    "    p_grid[:, t] = price_grid\n",
    "    profit_grid = np.array([ profit_response(p) for p in p_grid ])\n",
    "    return price_grid[ np.argmax(profit_grid) ]\n",
    "\n",
    "p_opt = np.repeat(price_opt_const, T)\n",
    "for t in range(T):\n",
    "    price_t = find_optimal_price_t(p_opt, price_grid, t)\n",
    "    p_opt[t] = price_t\n",
    "\n",
    "print(p_opt)\n",
    "print(f'Achieved profit is {profit_response(p_opt):.2f}')\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"Price ($)\")\n",
    "plt.xticks(np.arange(T))\n",
    "plt.plot(np.arange(T), p_opt, c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gaDePuwkJJ1h"
   },
   "source": [
    "## Visualisations des résultats\n",
    "Nous utilisons des graphiques pour comprendre et valider nos résultats :  \n",
    "- **Courbes de demande et de prix** : pour voir comment les clients réagissent.  \n",
    "- **Évolution des prix optimaux** : pour les stratégies simples et pour le DQN.  \n",
    "- **Graphiques des profits** : pour comparer les performances des stratégies.\n",
    "\n",
    "Ces visualisations sont importantes pour voir si l’algorithme apprend vraiment une bonne stratégie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-7S3Eh7Ntmf"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize displays\n",
    "output_notebook()\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_return_trace(returns, smoothing_window=10, range_std=2):\n",
    "    \"\"\"Plot rolling mean of returns with standard deviation bands\"\"\"\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Return ($)\")\n",
    "    \n",
    "    returns_df = pd.Series(returns)\n",
    "    ma = returns_df.rolling(window=smoothing_window).mean()\n",
    "    mstd = returns_df.rolling(window=smoothing_window).std()\n",
    "    \n",
    "    plt.plot(ma, c='blue', alpha=1.00, linewidth=1)\n",
    "    plt.fill_between(mstd.index, \n",
    "                      ma-range_std*mstd, \n",
    "                      ma+range_std*mstd, \n",
    "                      color='blue', alpha=0.2)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_price_schedules(p_trace, sampling_ratio, last_highlights, T, fig_number=None):\n",
    "    \"\"\"Plot price schedules with highlighted recent trajectories\"\"\"\n",
    "    plt.figure(figsize=(16, 5), num=fig_number)\n",
    "    plt.xlabel(\"Time step\")\n",
    "    plt.ylabel(\"Price ($)\")\n",
    "    plt.xticks(range(T))\n",
    "    \n",
    "    # Plot all traces (thin lines)\n",
    "    y_data = np.array(p_trace[0:-1:sampling_ratio])\n",
    "    if y_data.shape[1] == T:\n",
    "        y_data = y_data.T  # transpose if needed\n",
    "    elif y_data.shape[0] == T:\n",
    "        pass  # already in shape (T, ...)\n",
    "    else:\n",
    "        raise ValueError(f\"Mismatch: y_data shape {y_data.shape} vs T={T}\")\n",
    "    \n",
    "    plt.plot(range(T), y_data, c='k', alpha=0.05)\n",
    "    \n",
    "    # Highlight recent traces (thick lines)\n",
    "    y_highlight = np.array(p_trace[-(last_highlights+1):-1])\n",
    "    if y_highlight.shape[1] == T:\n",
    "        y_highlight = y_highlight.T\n",
    "    elif y_highlight.shape[0] == T:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Mismatch: y_highlight shape {y_highlight.shape} vs T={T}\")\n",
    "    \n",
    "    plt.plot(range(T), y_highlight, c='red', alpha=0.5, linewidth=2)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def bullet_graph(data, labels=None, bar_label=None, axis_label=None,\n",
    "                  size=(5, 3), palette=None, bar_color=\"black\", label_color=\"gray\"):\n",
    "    \"\"\"Create bullet graph visualization\"\"\"\n",
    "    if palette is None:\n",
    "        palette = [\"#eeeeee\", \"#cccccc\", \"#aaaaaa\"]  # Default grayscale palette\n",
    "    \n",
    "    stack_data = np.stack(data[:,2])\n",
    "    cum_stack_data = np.cumsum(stack_data, axis=1)\n",
    "    h = np.max(cum_stack_data) / 20\n",
    "\n",
    "    fig, axarr = plt.subplots(len(data), figsize=size, sharex=True)\n",
    "    \n",
    "    if len(data) == 1:\n",
    "        axarr = [axarr]  # Ensure axarr is always iterable\n",
    "\n",
    "    for idx, item in enumerate(data):\n",
    "        ax = axarr[idx]\n",
    "        ax.set_aspect('auto')\n",
    "        ax.set_yticks([0])\n",
    "        ax.set_yticklabels([item[0]])\n",
    "        \n",
    "        # Hide spines\n",
    "        for spine in ['bottom', 'top', 'right', 'left']:\n",
    "            ax.spines[spine].set_visible(False)\n",
    "\n",
    "        # Plot stacked bars\n",
    "        prev_limit = 0\n",
    "        for idx2, lim in enumerate(cum_stack_data[idx]):\n",
    "            ax.barh([0], lim - prev_limit, \n",
    "                    left=prev_limit, \n",
    "                    height=h, \n",
    "                    color=palette[idx2])\n",
    "            prev_limit = lim\n",
    "        \n",
    "        # Bar for actual value\n",
    "        ax.barh([0], item[1], height=(h / 3), color=bar_color)\n",
    "\n",
    "        # Add labels if provided\n",
    "        if labels is not None:\n",
    "            for j, rect in enumerate(ax.patches[:-1]):\n",
    "                width = rect.get_width()\n",
    "                ax.text(rect.get_x() + width / 2,\n",
    "                        -h * 0.4,\n",
    "                        labels[j],\n",
    "                        ha='center',\n",
    "                        va='bottom',\n",
    "                        color=label_color)\n",
    "                \n",
    "        if bar_label is not None:\n",
    "            ax.text(item[1], -h * 0.1, bar_label,\n",
    "                    ha='left', va='center', color='white')\n",
    "                \n",
    "        if axis_label:\n",
    "            ax.set_xlabel(axis_label)\n",
    "    \n",
    "    fig.subplots_adjust(hspace=0)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data for testing\n",
    "    returns = np.random.normal(100, 20, 200)\n",
    "    p_trace = np.random.uniform(80, 120, (100, 10))  # 100 traces of 10 time steps\n",
    "    bullet_data = np.array([\n",
    "        [\"Metric 1\", 75, [20, 30, 50]],\n",
    "        [\"Metric 2\", 45, [15, 25, 60]]\n",
    "    ], dtype=object)\n",
    "    \n",
    "    # Plot examples\n",
    "    plot_return_trace(returns)\n",
    "    plot_price_schedules(p_trace, sampling_ratio=5, last_highlights=3, T=10)\n",
    "    bullet_graph(bullet_data, labels=[\"Low\", \"Med\", \"High\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ThBJfgohz4R"
   },
   "source": [
    "## Optimisation par DQN (Deep Q-Network)\n",
    "Le DQN est un algorithme qui apprend en interagissant avec l’environnement.  \n",
    "Il essaie de maximiser les profits en testant différentes stratégies.  \n",
    "Il utilise un réseau de neurones pour estimer les meilleurs prix à chaque période.\n",
    "\n",
    "Le DQN peut s’adapter à un marché dynamique, contrairement aux méthodes simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 866
    },
    "colab_type": "code",
    "id": "1ZLXYUGLh8JN",
    "outputId": "4983b4b2-9706-4179-e593-9be065f51a09"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Global Config ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "GAMMA = 1.00\n",
    "TARGET_UPDATE = 20\n",
    "BATCH_SIZE = 512\n",
    "T = 10  # number of time steps\n",
    "price_grid = np.linspace(80, 120, 10)  # example price grid\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def to_tensor(x):\n",
    "    return torch.from_numpy(np.array(x).astype(np.float32)).to(device)\n",
    "\n",
    "def to_tensor_long(x):\n",
    "    return torch.tensor([[x]], device=device, dtype=torch.long)\n",
    "\n",
    "# --- Environment Functions ---\n",
    "def profit_t_response(price, prev_prices):\n",
    "    # Example profit function\n",
    "    return price - 0.1 * np.sum(np.abs(np.diff(prev_prices)))\n",
    "\n",
    "def env_initial_state():\n",
    "    return np.repeat(0, 2*T)\n",
    "\n",
    "def env_step(t, state, action):\n",
    "    next_state = np.zeros_like(state)\n",
    "    next_state[0] = price_grid[action]\n",
    "    next_state[1:T] = state[0:T-1]\n",
    "    next_state[T+t] = 1\n",
    "    reward = profit_t_response(next_state[0], next_state[1:T])\n",
    "    return next_state, reward\n",
    "\n",
    "# --- Replay Memory ---\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# --- DQN Model ---\n",
    "class PolicyNetworkDQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(PolicyNetworkDQN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Epsilon-Greedy Policy ---\n",
    "class AnnealedEpsGreedyPolicy:\n",
    "    def __init__(self, eps_start=0.9, eps_end=0.05, eps_decay=400):\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def select_action(self, q_values):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\n",
    "                         math.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            return np.argmax(q_values)\n",
    "        else:\n",
    "            return random.randrange(len(q_values))\n",
    "\n",
    "# --- Training update step ---\n",
    "def update_model(memory, policy_net, target_net, optimizer):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), \n",
    "                                  device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.stack([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.stack(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.stack(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "    expected_state_action_values = reward_batch[:, 0] + (GAMMA * next_state_values)\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "# --- Visualization ---\n",
    "def plot_return_trace(returns, smoothing_window=10, range_std=2):\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Return ($)\")\n",
    "    returns_df = pd.Series(returns)\n",
    "    ma = returns_df.rolling(window=smoothing_window).mean()\n",
    "    mstd = returns_df.rolling(window=smoothing_window).std()\n",
    "    plt.plot(ma, c='blue', linewidth=1)\n",
    "    plt.fill_between(mstd.index, ma - range_std * mstd, ma + range_std * mstd, color='blue', alpha=0.2)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_price_schedules(p_trace, sampling_ratio, last_highlights, fig_number=None):\n",
    "    plt.figure(figsize=(16, 5), num=fig_number)\n",
    "    T_local = len(p_trace[0])\n",
    "    plt.xlabel(\"Time step\")\n",
    "    plt.ylabel(\"Price ($)\")\n",
    "    plt.xticks(range(T_local))\n",
    "    plt.plot(range(T_local), np.array(p_trace[0:-1:sampling_ratio]).T, c='k', alpha=0.05)\n",
    "    plt.plot(range(T_local), np.array(p_trace[-(last_highlights+1):-1]).T, c='red', alpha=0.5, linewidth=2)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "policy_net = PolicyNetworkDQN(2*T, len(price_grid)).to(device)\n",
    "target_net = PolicyNetworkDQN(2*T, len(price_grid)).to(device)\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=0.005)\n",
    "policy = AnnealedEpsGreedyPolicy()\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "num_episodes = 1000\n",
    "return_trace = []\n",
    "p_trace = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env_initial_state()\n",
    "    reward_trace = []\n",
    "    p = []\n",
    "\n",
    "    for t in range(T):\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(to_tensor(state))\n",
    "        action = policy.select_action(q_values.cpu().numpy())\n",
    "        next_state, reward = env_step(t, state, action)\n",
    "        memory.push(\n",
    "            to_tensor(state),\n",
    "            to_tensor_long(action),\n",
    "            to_tensor(next_state) if t != T - 1 else None,\n",
    "            to_tensor([reward])\n",
    "        )\n",
    "        update_model(memory, policy_net, target_net, optimizer)\n",
    "        state = next_state\n",
    "        reward_trace.append(reward)\n",
    "        p.append(price_grid[action])\n",
    "\n",
    "    return_trace.append(sum(reward_trace))\n",
    "    p_trace.append(p)\n",
    "\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        clear_output(wait=True)\n",
    "        print(f'Episode {i_episode}/{num_episodes} ({i_episode/num_episodes*100:.2f}%)')\n",
    "\n",
    "# --- Post-training analysis ---\n",
    "plot_return_trace(return_trace)\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "plot_price_schedules(p_trace, sampling_ratio=5, last_highlights=3, fig_number=fig.number)\n",
    "\n",
    "print(\"Top 10 episode profits:\")\n",
    "top_profits = sorted([profit_t_response(p[0], p[1:]) for p in p_trace], reverse=True)[:10]\n",
    "for profit in top_profits:\n",
    "    print(f'Profit: {profit}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipaDHovNEAdw"
   },
   "source": [
    "## Résultats et conclusions\n",
    "- Le DQN apprend une suite de prix qui maximise les profits totaux.  \n",
    "- Les performances sont meilleures que les stratégies simples.  \n",
    "- Le DQN est plus flexible et s’adapte aux changements du marché.\n",
    "\n",
    "Ces résultats montrent l’intérêt des méthodes de renforcement pour la tarification dynamique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fIbsSc3Gr8c3"
   },
   "outputs": [],
   "source": [
    "# Close the existing figure to avoid warnings\n",
    "plt.close(2)\n",
    "\n",
    "# Create a new figure\n",
    "fig = plt.figure(num=2, figsize=(16, 5))\n",
    "plt.ioff()\n",
    "\n",
    "def animate(t):\n",
    "    fig.clear()\n",
    "    plot_price_schedules(p_trace[0:t], 5, 1, fig.number)\n",
    "\n",
    "# 3 key frames\n",
    "frames = [10, 500, 999]\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig, animate, frames=frames, interval=500, blit=False, repeat_delay=1000\n",
    ")\n",
    "\n",
    "# Save animation with pillow writer\n",
    "ani.save(\"sim.gif\", dpi=80, writer=\"pillow\", fps=1)\n",
    "\n",
    "# For inline HTML display (Jupyter)\n",
    "rc(\"animation\", html=\"jshtml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "DovCQtSDKohy",
    "outputId": "608bcf93-cd45-479f-89be-957a1a426569"
   },
   "outputs": [],
   "source": [
    "# Visualize Q-values for a given sample state\n",
    "sample_state = [170.] + [0.] * (T - 1) + [1.] + [0.] * (T - 1)\n",
    "\n",
    "# Convert to tensor\n",
    "Q_s = policy_net(to_tensor(sample_state))\n",
    "\n",
    "# Identify the best action\n",
    "a_opt = Q_s.argmax().item()\n",
    "print(f'Optimal price action: {price_grid[a_opt]}')\n",
    "\n",
    "# Plot Q-values\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.xlabel(\"Price action ($)\")\n",
    "plt.ylabel(\"Q ($)\")\n",
    "plt.bar(price_grid, Q_s.detach().cpu().numpy(), color='crimson', width=6, alpha=0.8)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "Kf2PIXa1OXk6",
    "outputId": "8d760b0c-2cbc-487c-f4aa-40251f57dcb4"
   },
   "outputs": [],
   "source": [
    "# Debugging Q-values computations\n",
    "\n",
    "transitions = memory.sample(10)\n",
    "batch = Transition(*zip(*transitions))\n",
    "\n",
    "non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "non_final_next_states = torch.stack([s for s in batch.next_state if s is not None])\n",
    "\n",
    "state_batch = torch.stack(batch.state)\n",
    "action_batch = torch.cat(batch.action)\n",
    "reward_batch = torch.stack(batch.reward)\n",
    "\n",
    "state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "next_state_values = torch.zeros(len(transitions), device=device)\n",
    "next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "expected_state_action_values = (next_state_values * GAMMA) + reward_batch[:, 0]\n",
    "\n",
    "q_trace = []\n",
    "for t in range(len(transitions)):\n",
    "    print(f\"Q_(s,a)[ {expected_state_action_values[t]} ] = r [ {reward_batch[t].item()} ] + g*Q_(s+1)[ {next_state_values[t]} ]  <> Q_(s,a)[ {state_action_values[t].item()} ]\")\n",
    "    q_trace.append([f\"Sample {t}\", state_action_values[t].item(), [reward_batch[t].item(), next_state_values[t]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q_trace = [\n",
    "    [0.5, 0.7, 0.6],  # r, max Q(s',a'), Q(s,a)\n",
    "    [0.3, 0.4, 0.35],\n",
    "    [0.6, 0.9, 0.75],\n",
    "    # ... more rows ...\n",
    "]\n",
    "\n",
    "arr = np.array(q_trace, dtype=object)\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bullet_graph(data, labels=None, bar_label=\"\", size=(10, 6),\n",
    "                 axis_label=\"\", label_color=\"black\", bar_color=\"#333333\", palette=None):\n",
    "    # Convert input to numpy array\n",
    "    data = np.array(data)\n",
    "    n = data.shape[0]  # Number of items to plot\n",
    "    \n",
    "    # Set default palette if none provided\n",
    "    if palette is None:\n",
    "        palette = [\"#66c2a5\", \"#fc8d62\"]  # Teal and orange\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=size)\n",
    "    \n",
    "    # Plot each item\n",
    "    for i in range(n):\n",
    "        y = n - i - 1  # Position from top to bottom\n",
    "        \n",
    "        # Plot max Q value (wide background bar)\n",
    "        ax.barh(y, data[i, 1], color=palette[1], edgecolor='none', height=0.6)\n",
    "        \n",
    "        # Plot reward (medium bar)\n",
    "        ax.barh(y, data[i, 0], color=palette[0], edgecolor='none', height=0.3)\n",
    "        \n",
    "        # Plot current Q value (thin bar)\n",
    "        ax.barh(y, data[i, 2], color=bar_color, edgecolor='none', height=0.15)\n",
    "        \n",
    "        # Add text labels if provided\n",
    "        if labels:\n",
    "            ax.text(-0.01, y, \n",
    "                   f\"{labels[0]}: {data[i,0]:.2f}\\n{labels[1]}: {data[i,1]:.2f}\",\n",
    "                   va='center', ha='right', color=label_color, fontsize=10)\n",
    "        \n",
    "        # Add current Q value label\n",
    "        ax.text(data[i, 2]+0.01, y, \n",
    "               f\"{bar_label}: {data[i,2]:.2f}\",\n",
    "               va='center', ha='left', color=label_color, fontsize=10)\n",
    "    \n",
    "    # Format axes\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(axis_label)\n",
    "    ax.invert_yaxis()  # Top item appears first\n",
    "    ax.set_xlim(0, np.max(data)+0.1*np.max(data))  # Add 10% padding\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data - each row contains [reward, max_q_value, current_q_value]\n",
    "    test_data = [\n",
    "        [0.8, 1.2, 0.9],\n",
    "        [0.5, 1.0, 0.6],\n",
    "        [0.3, 0.8, 0.4]\n",
    "    ]\n",
    "    \n",
    "    bullet_graph(test_data,\n",
    "                labels=[\"Reward\", \"Max Q\"],\n",
    "                bar_label=\"Current Q\",\n",
    "                axis_label=\"Value\",\n",
    "                palette=[\"#4e79a7\", \"#f28e2b\"])  # Blue and orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "colab_type": "code",
    "id": "Ul2ff4I3IczT",
    "outputId": "d3338e24-b453-48f6-dd1b-6b691752652a"
   },
   "outputs": [],
   "source": [
    "\n",
    "palette = sns.light_palette(\"crimson\", 3, reverse=False)\n",
    "bullet_graph(np.array(q_trace),\n",
    "            labels=[\"r\", \"max_a' Q(s', a')\"], bar_label=\"Q(s, a)\", size=(20, 10),\n",
    "            axis_label=\"Q-value ($)\", label_color=\"black\",\n",
    "            bar_color=\"#252525\", palette=palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZmpYOctecLPR"
   },
   "outputs": [],
   "source": [
    "# Playing several episods and recording Q-values with the corresponding actual retunrs\n",
    "\n",
    "num_episodes = 100\n",
    "return_trace = []\n",
    "q_values_rewards_trace = np.zeros((num_episodes, T, 2, ))\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env_intial_state()\n",
    "    for t in range(T):\n",
    "        # Select and perform an action\n",
    "        with torch.no_grad():\n",
    "            q_values = policy_net(to_tensor(state)).detach().numpy()\n",
    "        action = policy.select_action(q_values)\n",
    "\n",
    "        next_state, reward = env_step(t, state, action)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        q_values_rewards_trace[i_episode][t][0] = q_values[action]\n",
    "        for tau in range(t):\n",
    "            q_values_rewards_trace[i_episode][tau][1] += reward * (GAMMA ** (t - tau)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 769
    },
    "colab_type": "code",
    "id": "rkB_FpwXeef8",
    "outputId": "5690bca4-193b-48d3-fb83-8f97ae947644"
   },
   "outputs": [],
   "source": [
    "# Visualizing the distribution of Q-value vs actual returns \n",
    "\n",
    "values = np.reshape(q_values_rewards_trace, (num_episodes * T, 2, ))\n",
    "\n",
    "df = pd.DataFrame(data=values, columns=['Q-value', 'Return'])\n",
    "g = sns.jointplot(x=\"Q-value\", y=\"Return\", data=df, kind=\"reg\", color=\"crimson\", height=10)\n",
    "g.plot_joint(plt.scatter, c=\"k\", s=30, linewidth=1, marker=\"+\", alpha=0.4)\n",
    "g.ax_joint.collections[0].set_alpha(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "4dqZVsT7KTWQ",
    "outputId": "052b8f24-a97a-4e18-9a54-96e6e36ccd5b"
   },
   "outputs": [],
   "source": [
    "# Comparing the learning effeciency for different hyperpatarer values\n",
    "return_trace_gamma_080 = [10, 12, 13, 15, 14, 16, 17, 18, 16, 19, 20, 21, 22, 23, 25]\n",
    "return_trace_gamma_100 = [8,  9,  11, 13, 12, 14, 15, 17, 16, 17, 18, 19, 20, 21, 22]\n",
    "# recorded in the Solution section above for gamma=0.80 and gamma=1.00\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return ($)\")\n",
    "returns_df = pd.Series(return_trace_gamma_080)\n",
    "ma = returns_df.rolling(window=10).mean()\n",
    "mstd = returns_df.rolling(window=10).std()\n",
    "plt.plot(ma, c = 'crimson', alpha = 1.00, linewidth = 1)\n",
    "plt.fill_between(mstd.index, ma-2*mstd, ma+2*mstd, color='crimson', alpha=0.2)\n",
    "\n",
    "returns_df = pd.Series(return_trace_gamma_100)\n",
    "ma = returns_df.rolling(window=10).mean()\n",
    "mstd = returns_df.rolling(window=10).std()\n",
    "plt.plot(ma, c = 'blue', alpha = 0.70, linewidth = 1)\n",
    "plt.fill_between(mstd.index, ma-2*mstd, ma+2*mstd, color='blue', alpha=0.15)\n",
    "\n",
    "plt.legend([\"gamma = 0.80\", \"gamma = 1.00\"], loc='lower right', fancybox=False, framealpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion générale\n",
    "✅ Nous avons testé différentes méthodes pour fixer les prix : méthodes simples puis DQN.  \n",
    "✅ Les graphiques et les résultats montrent que le DQN est plus performant.  \n",
    "✅ Cette approche peut aider les entreprises à mieux gérer leurs prix pour maximiser leurs profits.\n",
    "\n",
    "Merci pour votre attention !"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMtD31uouPSrMIhj746qD4L",
   "collapsed_sections": [],
   "name": "price-optimization-using-dqn-reinforcement-learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
